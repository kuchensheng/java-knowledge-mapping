# Java的I/O演进

# 1. Linux网络I/O模型简介
Linux的内核将所有外部设备都看做一个文件，对一个文件的读写会调用内核系统的命令，返回一个fd（file descriptor，文件描述符）。根据UNIX网络编程对I/O模型的分类，UNIX提供了5种I/O模型：

+（1）阻塞I/O模型：最常用的I/O模型，缺省情形下，所有文件操作都是阻塞的。
+ (2) 非阻塞I/O模型：面对缓冲区，如果缓冲区没有数据，就返回一个EWOULDLOCK错误。一般都对非阻塞I/O模型进行轮询状态检查
+ (3) I/O复用模型：Linux提供select/poll，进程通过将一个或多个fd传递给select/poll系统调用，阻塞在select操作上，这样select/poll可以帮我们侦测多个发的是否处于就绪状态。select/poll是**顺序扫描**fd是否就绪，而且支持的fd数量有限。Linux还提供了一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback。
+ (4) 信号驱动I/O模型：首先开启套接字信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序来读取书序，并通知主循环函数处理数据。
+ (5) 异步I/O：通知内核启动某个操作，并让内核在整个操作完成后通知我们。这种模型与信号驱动模型的主要区别是：信号驱动I/O由内核通知我们何时可以开始一个I/O操作；异步I/O模型由内核通知我们操作何时完成。

# 2. I/O多路复用技术
I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。

目前支持I/O多路复用的系统调用有：select、pselect、poll、epoll。

epoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。现总结如下：

+ 支持一个进程打开的大数目的socket描述符。
select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认是1024.而epoll没有这个限制，它所支持的FD的上限是操作系统的最大文件句柄数。一般来说在1GB内存的机器上大约是10万个句柄左右。

+ I/O效率不会随着FD数据的增加而线性下滑。
传统select/poll的另一个弱点：当socket集合很大时，由于网络延时或者链路空闲，任一时刻只有少部分的socket是“活跃”的，但是select/poll每次调用都会限行扫描全部的集合，导致效率呈线性下降。epoll不会存在这个问题，它只会对“活跃”的socket进行操作——这是因为在内核实现中，epoll是根据每个fd上的callback函数实现的。只有“活跃”的socket才会去主动调用callback函数。

+ 使用mmap加速内核与用户空间的消息传递
无论是select、poll还是epoll都需要内核把FD消息通知给用户空间，epoll是通过内核和用户空间mmap同一块内存来实现的。

+ epoll的API更加简单
